{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Topic Modeling on Hindi Language.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfADJ6wBBiLo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6680542a-5586-452f-d06b-1f29fa4b7293"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LidxGMgpfrNi",
        "outputId": "815944e7-d3ac-4e8f-fe9b-75fd66c7c41b"
      },
      "source": [
        "!pip install torch==1.2.0+cu92 torchvision==0.4.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install inltk"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.2.0+cu92\n",
            "  Downloading https://download.pytorch.org/whl/cu92/torch-1.2.0%2Bcu92-cp37-cp37m-manylinux1_x86_64.whl (663.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 663.1 MB 1.8 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.4.0+cu92\n",
            "  Downloading https://download.pytorch.org/whl/cu92/torchvision-0.4.0%2Bcu92-cp37-cp37m-manylinux1_x86_64.whl (8.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.8 MB 41.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.2.0+cu92) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchvision==0.4.0+cu92) (1.15.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.4.0+cu92) (7.1.2)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.0+cu102\n",
            "    Uninstalling torch-1.9.0+cu102:\n",
            "      Successfully uninstalled torch-1.9.0+cu102\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.10.0+cu102\n",
            "    Uninstalling torchvision-0.10.0+cu102:\n",
            "      Successfully uninstalled torchvision-0.10.0+cu102\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.2.0+cu92 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.2.0+cu92 torchvision-0.4.0+cu92\n",
            "Collecting inltk\n",
            "  Downloading inltk-0.9-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from inltk) (2.23.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from inltk) (1.4.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from inltk) (4.6.3)\n",
            "Requirement already satisfied: fastprogress>=0.1.19 in /usr/local/lib/python3.7/dist-packages (from inltk) (1.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from inltk) (21.0)\n",
            "Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.7/dist-packages (from inltk) (7.352.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from inltk) (3.2.2)\n",
            "Requirement already satisfied: spacy>=2.0.18 in /usr/local/lib/python3.7/dist-packages (from inltk) (2.2.4)\n",
            "Collecting async-timeout>=3.0.1\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Collecting fastai==1.0.57\n",
            "  Downloading fastai-1.0.57-py3-none-any.whl (233 kB)\n",
            "\u001b[K     |████████████████████████████████| 233 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting typing\n",
            "  Downloading typing-3.7.4.3.tar.gz (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 6.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from inltk) (1.1.5)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from inltk) (2.7.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from inltk) (3.13)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from inltk) (1.19.5)\n",
            "Collecting aiohttp>=3.5.4\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 39.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: bottleneck in /usr/local/lib/python3.7/dist-packages (from inltk) (1.3.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from inltk) (7.1.2)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 37.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from fastai==1.0.57->inltk) (0.4.0+cu92)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from fastai==1.0.57->inltk) (1.2.0+cu92)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.5.4->inltk) (3.7.4.3)\n",
            "Requirement already satisfied: chardet<5.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.5.4->inltk) (3.0.4)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[K     |████████████████████████████████| 294 kB 50.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.5.4->inltk) (21.2.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 49.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->inltk) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->inltk) (2.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->inltk) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->inltk) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->inltk) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->inltk) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->inltk) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->inltk) (57.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->inltk) (4.62.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->inltk) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->inltk) (0.8.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.0.18->inltk) (4.8.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.0.18->inltk) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->inltk) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->inltk) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->inltk) (2.10)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->inltk) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->inltk) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->inltk) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->inltk) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->inltk) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->inltk) (2018.9)\n",
            "Building wheels for collected packages: typing\n",
            "  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26324 sha256=093b25cc630eeb53ccebb4b8c36d635937876188cbba4c4499c767d9a066d716\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/f3/15/01aa6571f0a72ee6ae7b827c1491c37a1f72d686fd22b43b0e\n",
            "Successfully built typing\n",
            "Installing collected packages: multidict, yarl, async-timeout, typing, sentencepiece, fastai, aiohttp, inltk\n",
            "  Attempting uninstall: fastai\n",
            "    Found existing installation: fastai 1.0.61\n",
            "    Uninstalling fastai-1.0.61:\n",
            "      Successfully uninstalled fastai-1.0.61\n",
            "Successfully installed aiohttp-3.7.4.post0 async-timeout-3.0.1 fastai-1.0.57 inltk-0.9 multidict-5.2.0 sentencepiece-0.1.96 typing-3.7.4.3 yarl-1.6.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "typing"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "mepEjQ1jfyWJ",
        "outputId": "ac58e196-d689-4356-f00a-33fc6668297b"
      },
      "source": [
        "from inltk.inltk import setup\n",
        "from inltk.inltk import tokenize\n",
        "from inltk.inltk import get_embedding_vectors\n",
        "setup('hi')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-50319985d675>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0minltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0minltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_embedding_vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hi'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/inltk/inltk.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(language_code)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mloop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_event_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mlearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/asyncio/base_events.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \"\"\"\n\u001b[1;32m    562\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_runnung\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0mnew_task\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfuture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/asyncio/base_events.py\u001b[0m in \u001b[0;36m_check_runnung\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_runnung\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'This event loop is already running'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m             raise RuntimeError(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: This event loop is already running"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNz-KwecCUeu"
      },
      "source": [
        "# import codecs\n",
        "# # import re\n",
        "# class Tokenizer():\n",
        "# \t'''class for tokenizer'''\n",
        "\n",
        "# \tdef __init__(self,text=None):\n",
        "# \t\tif text is  not None:\n",
        "# \t\t\tself.text=text.decode('utf-8')\n",
        "# \t\t\tself.clean_text()\n",
        "# \t\telse:\n",
        "# \t\t\tself.text=None\n",
        "# \t\tself.sentences=[]\n",
        "# \t\tself.tokens=[]\n",
        "# \t\tself.stemmed_word=[]\n",
        "# \t\tself.final_list=[]\n",
        "# \t\t#self.final_tokens=[]\n",
        "\t\n",
        "\n",
        "# \tdef read_from_file(self,filename):\n",
        "# \t\tf=codecs.open(filename,encoding='utf-8')\n",
        "# \t\tself.text=f.read()\n",
        "# \t\tself.clean_text()\n",
        "\n",
        "\n",
        "\n",
        "# \tdef generate_sentences(self):\n",
        "# \t\t'''generates a list of sentences'''\n",
        "# \t\ttext=self.text\n",
        "# \t\tself.sentences=text.split(u\"।\")\n",
        "\n",
        "# \tdef print_sentences(self,sentences=None):\n",
        "# \t\tif sentences:\n",
        "# \t\t\tfor i in sentences:\n",
        "# \t\t\t\tprint(i.encode('utf-8'))\n",
        "# \t\telse:\n",
        "# \t\t\tfor i in self.sentences:\n",
        "# \t\t\t\tprint(i.encode('utf-8'))\n",
        "\n",
        "\n",
        "# \tdef clean_text(self):\n",
        "# \t\t'''not working'''\n",
        "# \t\ttext=self.text\n",
        "# \t\ttext=re.sub(r'(\\d+)',r'',text)\n",
        "# \t\ttext=text.replace(u',','')\n",
        "# \t\ttext=text.replace(u'\"','')\n",
        "# \t\ttext=text.replace(u'(','')\n",
        "# \t\ttext=text.replace(u')','')\n",
        "# \t\ttext=text.replace(u'\"','')\n",
        "# \t\ttext=text.replace(u':','')\n",
        "# \t\ttext=text.replace(u\"'\",'')\n",
        "# \t\ttext=text.replace(u\"‘‘\",'')\n",
        "# \t\ttext=text.replace(u\"’’\",'')\n",
        "# \t\ttext=text.replace(u\"''\",'')\n",
        "# \t\ttext=text.replace(u\".\",'')\n",
        "# \t\tself.text=text\n",
        "\n",
        "# \tdef remove_only_space_words(self):\n",
        "\n",
        "# \t\ttokens=filter(lambda tok: tok.strip(),self.tokens)\n",
        "# \t\tself.tokens=tokens\n",
        "\t\t\n",
        "# \tdef hyphenated_tokens(self):\n",
        "\n",
        "# \t\tfor each in self.tokens:\n",
        "# \t\t\tif '-' in each:\n",
        "# \t\t\t\ttok=each.split('-')\n",
        "# \t\t\t\tself.tokens.remove(each)\n",
        "# \t\t\t\tself.tokens.append(tok[0])\n",
        "# \t\t\t\tself.tokens.append(tok[1])\n",
        "\n",
        "\n",
        "\n",
        "# \tdef tokenize(self):\n",
        "# \t\t'''done'''\n",
        "# \t\tif not self.sentences:\n",
        "# \t\t\tself.generate_sentences()\n",
        "\n",
        "# \t\tsentences_list=self.sentences\n",
        "# \t\ttokens=[]\n",
        "# \t\tfor each in sentences_list:\n",
        "# \t\t\tword_list=each.split(' ')\n",
        "# \t\t\ttokens=tokens+word_list\n",
        "# \t\tself.tokens=tokens\n",
        "# \t\t#remove words containing spaces\n",
        "# \t\tself.remove_only_space_words()\n",
        "# \t\t#remove hyphenated words\n",
        "# \t\tself.hyphenated_tokens()\n",
        "\n",
        "# \tdef print_tokens(self,print_list=None):\n",
        "# \t\t'''done'''\n",
        "# \t\tif print_list is None:\n",
        "# \t\t\tfor i in self.tokens:\n",
        "# \t\t\t\tprint(i.encode('utf-8'))\n",
        "# \t\telse:\n",
        "# \t\t\tfor i in print_list:\n",
        "# \t\t\t\tprint(i.encode('utf-8'))\n",
        "\n",
        "\n",
        "# \tdef tokens_count(self):\n",
        "# \t\t'''done'''\n",
        "# \t\treturn len(self.tokens)\n",
        "\n",
        "# \tdef sentence_count(self):\n",
        "# \t\t'''done'''\n",
        "# \t\treturn len(self.sentences)\n",
        "\n",
        "# \tdef len_text(self):\n",
        "# \t\t'''done'''\n",
        "# \t\treturn len(self.text)\n",
        "\n",
        "# \tdef concordance(self,word):\n",
        "# \t\t'''done'''\n",
        "# \t\tif not self.sentences:\n",
        "# \t\t\tself.generate_sentences()\n",
        "# \t\tsentence=self.sentences\n",
        "# \t\tconcordance_sent=[]\n",
        "# \t\tfor each in sentence:\n",
        "# \t\t\teach=each.encode('utf-8')\n",
        "# \t\t\tif word in each:\n",
        "# \t\t\t\tconcordance_sent.append(each.decode('utf-8'))\n",
        "# \t\treturn concordance_sent\n",
        "\n",
        "# \tdef generate_freq_dict(self):\n",
        "# \t\t'''done'''\n",
        "# \t\tfreq={}\n",
        "# \t\tif not self.tokens:\n",
        "# \t\t\tself.tokenize()\n",
        "\n",
        "# \t\ttemp_tokens=self.tokens\n",
        "# \t\t#doubt whether set can be used here or not\n",
        "# \t\tfor each in self.tokens:\n",
        "# \t\t\tfreq[each]=temp_tokens.count(each)\n",
        "\n",
        "# \t\treturn freq\n",
        "\n",
        "# \tdef print_freq_dict(self,freq):\n",
        "# \t\t'''done'''\n",
        "# \t\tfor i in freq.keys():\n",
        "# \t\t\tprint(i.encode('utf-8'),',',freq[i])\n",
        "\n",
        "# \tdef generate_stem_words(self,word):\n",
        "# \t\tsuffixes = {\n",
        "#     1: [u\"ो\",u\"े\",u\"ू\",u\"ु\",u\"ी\",u\"ि\",u\"ा\"],\n",
        "#     2: [u\"कर\",u\"ाओ\",u\"िए\",u\"ाई\",u\"ाए\",u\"ने\",u\"नी\",u\"ना\",u\"ते\",u\"ीं\",u\"ती\",u\"ता\",u\"ाँ\",u\"ां\",u\"ों\",u\"ें\"],\n",
        "#     3: [u\"ाकर\",u\"ाइए\",u\"ाईं\",u\"ाया\",u\"ेगी\",u\"ेगा\",u\"ोगी\",u\"ोगे\",u\"ाने\",u\"ाना\",u\"ाते\",u\"ाती\",u\"ाता\",u\"तीं\",u\"ाओं\",u\"ाएं\",u\"ुओं\",u\"ुएं\",u\"ुआं\"],\n",
        "#     4: [u\"ाएगी\",u\"ाएगा\",u\"ाओगी\",u\"ाओगे\",u\"एंगी\",u\"ेंगी\",u\"एंगे\",u\"ेंगे\",u\"ूंगी\",u\"ूंगा\",u\"ातीं\",u\"नाओं\",u\"नाएं\",u\"ताओं\",u\"ताएं\",u\"ियाँ\",u\"ियों\",u\"ियां\"],\n",
        "#     5: [u\"ाएंगी\",u\"ाएंगे\",u\"ाऊंगी\",u\"ाऊंगा\",u\"ाइयाँ\",u\"ाइयों\",u\"ाइयां\"],\n",
        "# }\n",
        "# \t\tfor L in 5, 4, 3, 2, 1:\n",
        "# \t\t\tif len(word) > L + 1:\n",
        "# \t\t\t\tfor suf in suffixes[L]:\n",
        "# \t\t\t\t\t#print type(suf),type(word),word,suf\n",
        "# \t\t\t\t\tif word.endswith(suf):\n",
        "# \t\t\t\t\t\t#print 'h'\n",
        "# \t\t\t\t\t\treturn word[:-L]\n",
        "# \t\treturn word\n",
        "\n",
        "# \tdef generate_stem_dict(self):\n",
        "# \t\t'''returns a dictionary of stem words for each token'''\n",
        "# \t\t# suffixes = {\n",
        "#   #   \t\t\t\t1: [\"ो\", \"े\", \"ू\", \"ु\", \"ी\", \"ि\", \"ा\"],\n",
        "#   #   \t\t\t\t2: [\"कर\", \"ाओ\", \"िए\", \"ाई\", \"ाए\", \"ने\", \"नी\", \"ना\", \"ते\", \"ीं\", \"ती\", \"ता\", \"ाँ\", \"ां\", \"ों\", \"ें\"],\n",
        "#   #   \t\t\t\t3: [\"ाकर\", \"ाइए\", \"ाईं\", \"ाया\", \"ेगी\", \"ेगा\", \"ोगी\", \"ोगे\", \"ाने\", \"ाना\", \"ाते\", \"ाती\", \"ाता\", \"तीं\", \"ाओं\", \"ाएं\", \"ुओं\", \"ुएं\", \"ुआं\"],\n",
        "#   #   \t\t\t\t4: [\"ाएगी\", \"ाएगा\", \"ाओगी\", \"ाओगे\", \"एंगी\", \"ेंगी\", \"एंगे\", \"ेंगे\", \"ूंगी\", \"ूंगा\", \"ातीं\", \"नाओं\", \"नाएं\", \"ताओं\", \"ताएं\", \"ियाँ\", \"ियों\", \"ियां\"],\n",
        "#   #   \t\t\t\t5: [\"ाएंगी\", \"ाएंगे\", \"ाऊंगी\", \"ाऊंगा\", \"ाइयाँ\", \"ाइयों\", \"ाइयां\"],\n",
        "# \t\t# \t\t\t}\n",
        "\n",
        "# \t\tstem_word={}\n",
        "# \t\tif not self.tokens:\n",
        "# \t\t\tself.tokenize()\n",
        "# \t\tfor each_token in self.tokens:\n",
        "# \t\t\t#print type(each_token)\n",
        "# \t\t\ttemp=self.generate_stem_words(each_token)\n",
        "# \t\t\t#print temp\n",
        "# \t\t\tstem_word[each_token]=temp\n",
        "# \t\t\tself.stemmed_word.append(temp)\n",
        "\t\t\t\n",
        "# \t\treturn stem_word\n",
        "\n",
        "# \tdef remove_stop_words(self):\n",
        "# \t\tf=codecs.open(\"rss.txt\",encoding='utf-8')\n",
        "# \t\tif not self.stemmed_word:\n",
        "# \t\t\tself.generate_stem_dict()\n",
        "# \t\tstopwords=[x.strip() for x in f.readlines()]\n",
        "# \t\ttokens=[i for i in self.stemmed_word if unicode(i) not in stopwords]\n",
        "# \t\tself.final_tokens=tokens\n",
        "# \t\treturn tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwGTHVVpFDbV",
        "outputId": "fd26c6c1-d65d-46a9-cfcb-abe12138c629"
      },
      "source": [
        "import pandas as pd\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/Maths Problem Solving BTP/Hindi_Datasets.csv')\n",
        "# data_text = train_df['Problem ']\n",
        "topic_df = train_df['Topic']\n",
        "train_df = train_df.drop(['Topic'], axis = 1)\n",
        "train_df = train_df.drop(['Answer'], axis = 1)\n",
        "train_df['Index'] = train_df.index\n",
        "documents = train_df\n",
        "print(len(documents))\n",
        "print(documents[:5])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "51\n",
            "                                             Problem  Index\n",
            "0  हर छोटी बस में 35 बच्चे बैठ सकते हैं। 1050 बच्...      0\n",
            "1  बशीर के पास 100 रुपये हैं। उसने एक-चौथाई पैसे ...      1\n",
            "2  बशीर के पास 100 रुपये हैं। उसने एक-चौथाई पैसे ...      2\n",
            "3  ग्रेसी ने जाल खरीदने वेफ लिए 4000 रुपये का कर्...      3\n",
            "4  झाँसी और उसकी बहन ने 21000 रुपये का कर्श नाव क...      4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gN9fZ1FOd_j4",
        "outputId": "2793d312-ed1e-4b04-c949-4b7518b3c7a2"
      },
      "source": [
        "stopwords_df = pd.read_csv('/content/drive/MyDrive/Maths Problem Solving BTP/Hindi_Stopwords.csv')\n",
        "temp_list = stopwords_df.values.tolist()\n",
        "stopwords_list = []\n",
        "for i in temp_list:\n",
        "  stopword = i[0]\n",
        "  stopwords_list.append(stopword)\n",
        "print(stopwords_list)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['मैं', 'मुझको', 'मेरा', 'अपने', 'आप', 'को', 'हमने', 'हमारा', 'अपना', 'हम', 'आपका', 'तुम्हारा', 'स्वयं', 'वह', 'इसे', 'उसके', 'कि', 'वह', 'उसकी', 'उसका', 'यह', 'इसके', 'उन्होने', 'अपने', 'जो', 'कि', 'ये', 'हूँ', 'रहे', 'थी', 'थे', 'होना', 'गया', 'है', 'पडा', 'होने', 'करना', 'करता है', 'किया', 'रही', 'एक', 'लेकिन', 'अगर', 'या', 'क्यूंकि', 'जैसा', 'जब तक', 'जबकि', 'की', 'पर', 'द्वारा', 'के लिए', 'साथ', 'के बारे में', 'खिलाफ', 'बीच', 'में', 'के माध्यम से', 'दौरान', 'से पहले', 'के बाद', 'ऊपर', 'नीचे', 'को', 'से', 'तक', 'नीचे', 'निकल', 'बंद', 'से', 'तहत', 'दुबारा', 'आगे', 'फिर', 'बार', 'यहाँ', 'वहाँ', 'कब', 'कहाँ', 'क्यों', 'कैसे', 'सारे', 'किसी', 'दोनो', 'प्रत्येक', 'ज्यादा', 'अधिकांश', 'अन्य', 'में', 'ऐसा', 'कोई', 'मात्र', 'खुद', 'समान', 'इसलिए', 'बहुत', 'सकता', 'जायेंगे', 'जरा', 'चाहिए', 'अभी', 'और', 'कर दिया', 'रखें', 'का', 'हैं', 'इस', 'होता', 'करने', 'ने', 'बनी', 'तो', 'ही', 'हो', 'इसका', 'था', 'हुआ', 'वाले', 'बाद', 'लिए', 'सकते', 'इसमें', 'दो', 'वे', 'करते', 'कहा', 'वर्ग', 'कई', 'करें', 'होती', 'अपनी', 'उनके', 'यदि', 'हुई', 'जा', 'कहते', 'जब', 'होते', 'कोई', 'हुए', 'व', 'जैसे', 'सभी', 'करता', 'उनकी', 'तरह', 'उस', 'आदि', 'इसकी', 'उनका', 'इसी', 'पे', 'तथा', 'भी', 'परंतु', 'इन', 'कम', 'दूर', 'पूरे', 'गये', 'तुम', 'मै', 'यहां', 'हुये', 'कभी', 'अथवा', 'गयी', 'प्रति', 'जाता', 'इन्हें', 'गई', 'अब', 'जिसमें', 'लिया', 'बड़ा', 'जाती', 'तब', 'उसे', 'जाते', 'लेकर', 'बड़े', 'दूसरे', 'जाने', 'बाहर', 'स्थान', 'उन्हें', 'गए', 'ऐसे', 'जिससे', 'समय', 'दोनों', 'किए', 'रहती', 'इनके', 'इनका', 'इनकी', 'सकती', 'आज', 'कल', 'जिन्हें', 'जिन्हों', 'तिन्हें', 'तिन्हों', 'किन्हों', 'किन्हें', 'इत्यादि', 'इन्हों', 'उन्हों', 'बिलकुल', 'निहायत', 'इन्हीं', 'उन्हीं', 'साबुत', 'वग़ैरह', 'लिये', 'दिया', 'जिसे', 'तिसे', 'काफ़ी', 'पहले', 'बाला', 'मानो', 'अंदर', 'भीतर', 'पूरा', 'सारा', 'उनको', 'वहीं', 'जहाँ', 'जीधर', 'के', 'एवं', 'कुछ', 'कुल', 'रहा', 'जिस', 'जिन', 'तिस', 'तिन', 'कौन', 'किस', 'संग', 'यही', 'बही', 'उसी', 'मगर', 'कर', 'मे', 'एस', 'उन', 'सो', 'अत']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWXT46NsaXft"
      },
      "source": [
        "import snowballstemmer\n",
        "\n",
        "stemmer = snowballstemmer.stemmer('hindi');\n",
        "def lemmatize_stemming(text):\n",
        "    word = stemmer.stemWords(text.split())\n",
        "    return word\n",
        "\n",
        "def preprocess(text):\n",
        "    result = []\n",
        "    tokens = text.split(' ')\n",
        "    # tokens = tokenize(text, 'hi')\n",
        "    for token in tokens:\n",
        "      # if not isinstance(token, str):\n",
        "      #   token = str(token)\n",
        "      print(token)\n",
        "      if token not in stopwords_list:\n",
        "        lemmatized_token = lemmatize_stemming(token)\n",
        "        print(lemmatized_token)\n",
        "        result.append(lemmatized_token[0])\n",
        "    return result"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WSkv6lUacUb",
        "outputId": "c9566c12-a31c-4ca3-fd88-0c6d87669862"
      },
      "source": [
        "text = \"हर छोटी बस में 1/5\"\n",
        "\n",
        "print(preprocess(text))\n",
        "text = \"हर छोटी बस में 500\"\n",
        "\n",
        "print(preprocess(text))\n",
        "text = \"हर छोटी बस में 500.03\"\n",
        "\n",
        "print(preprocess(text))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "हर\n",
            "छोटी\n",
            "बस\n",
            "में\n",
            "1/5\n",
            "['हर', 'छोट', 'बस', '1/5']\n",
            "हर\n",
            "छोटी\n",
            "बस\n",
            "में\n",
            "500\n",
            "['हर', 'छोट', 'बस', '500']\n",
            "हर\n",
            "छोटी\n",
            "बस\n",
            "में\n",
            "500.03\n",
            "['हर', 'छोट', 'बस', '500.03']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rml8aWl-ivUP",
        "outputId": "fab17057-3304-40e4-d456-045655b42f0b"
      },
      "source": [
        "# print(documents)\n",
        "\n",
        "processed_docs = documents['Problem'].map(preprocess)\n",
        "processed_docs[:10]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "हर\n",
            "['हर']\n",
            "छोटी\n",
            "['छोट']\n",
            "बस\n",
            "['बस']\n",
            "में\n",
            "35\n",
            "['35']\n",
            "बच्चे\n",
            "['बच्च']\n",
            "बैठ\n",
            "['बैठ']\n",
            "सकते\n",
            "हैं।\n",
            "['हैं।']\n",
            "1050\n",
            "['1050']\n",
            "बच्चों\n",
            "['बच्च']\n",
            "बैठने\n",
            "['बैठ']\n",
            "के\n",
            "लिए\n",
            "कितनी\n",
            "['कित']\n",
            "बसों\n",
            "['बस']\n",
            "की\n",
            "ज़्रूरत\n",
            "['ज़्रूरत']\n",
            "होगी?\n",
            "['होगी?']\n",
            "बशीर\n",
            "['बशीर']\n",
            "के\n",
            "पास\n",
            "['पास']\n",
            "100\n",
            "['100']\n",
            "रुपये\n",
            "['रुपय']\n",
            "हैं।\n",
            "['हैं।']\n",
            "उसने\n",
            "['उस']\n",
            "एक-चौथाई\n",
            "['एक-चौथ']\n",
            "पैसे\n",
            "['पैस']\n",
            "‘स्क्वीड’\n",
            "['‘स्क्वीड’']\n",
            "पर\n",
            "खर्च\n",
            "['खर्च']\n",
            "किए\n",
            "और\n",
            "तीन-चौथाई\n",
            "['तीन-चौथ']\n",
            "पैसे\n",
            "['पैस']\n",
            "‘झींगा\n",
            "['‘झींग']\n",
            "मछली’\n",
            "['मछली’']\n",
            "पर\n",
            "खर्च\n",
            "['खर्च']\n",
            "किए।\n",
            "['किए।']\n",
            "उसने\n",
            "['उस']\n",
            "कितने\n",
            "['कित']\n",
            "किलोग्राम\n",
            "['किलोग्राम']\n",
            "स्क्वीड\n",
            "['स्क्वीड']\n",
            "खरीदी?\n",
            "['खरीदी?']\n",
            "बशीर\n",
            "['बशीर']\n",
            "के\n",
            "पास\n",
            "['पास']\n",
            "100\n",
            "['100']\n",
            "रुपये\n",
            "['रुपय']\n",
            "हैं।\n",
            "['हैं।']\n",
            "उसने\n",
            "['उस']\n",
            "एक-चौथाई\n",
            "['एक-चौथ']\n",
            "पैसे\n",
            "['पैस']\n",
            "‘स्क्वीड’\n",
            "['‘स्क्वीड’']\n",
            "पर\n",
            "खर्च\n",
            "['खर्च']\n",
            "किए\n",
            "और\n",
            "तीन-चौथाई\n",
            "['तीन-चौथ']\n",
            "पैसे\n",
            "['पैस']\n",
            "‘झींगा\n",
            "['‘झींग']\n",
            "मछली’\n",
            "['मछली’']\n",
            "पर\n",
            "खर्च\n",
            "['खर्च']\n",
            "किए।\n",
            "['किए।']\n",
            "उसने\n",
            "['उस']\n",
            "कितने\n",
            "['कित']\n",
            "किलोग्राम\n",
            "['किलोग्राम']\n",
            "झींगा\n",
            "['झींग']\n",
            "मछली\n",
            "['मछल']\n",
            "खरीदी?\n",
            "['खरीदी?']\n",
            "ग्रेसी\n",
            "['ग्रेस']\n",
            "ने\n",
            "जाल\n",
            "['जाल']\n",
            "खरीदने\n",
            "['खरीद']\n",
            "वेफ\n",
            "['वेफ']\n",
            "लिए\n",
            "4000\n",
            "['4000']\n",
            "रुपये\n",
            "['रुपय']\n",
            "का\n",
            "कर्श\n",
            "['कर्श']\n",
            "लिया।\n",
            "['लिया।']\n",
            "उसने\n",
            "['उस']\n",
            "345\n",
            "['345']\n",
            "रुपये\n",
            "['रुपय']\n",
            "हर\n",
            "['हर']\n",
            "महीने\n",
            "['महीन']\n",
            "एक\n",
            "साल\n",
            "['साल']\n",
            "तक\n",
            "वापस\n",
            "['वापस']\n",
            "दिए।\n",
            "['दिए।']\n",
            "वह\n",
            "कितने\n",
            "['कित']\n",
            "पैसे\n",
            "['पैस']\n",
            "बैंक\n",
            "['बैंक']\n",
            "को\n",
            "वापस\n",
            "['वापस']\n",
            "करेगी?\n",
            "['करेगी?']\n",
            "झाँसी\n",
            "['झाँस']\n",
            "और\n",
            "उसकी\n",
            "बहन\n",
            "['बहन']\n",
            "ने\n",
            "21000\n",
            "['21000']\n",
            "रुपये\n",
            "['रुपय']\n",
            "का\n",
            "कर्श\n",
            "['कर्श']\n",
            "नाव\n",
            "['नाव']\n",
            "को\n",
            "खरीदने\n",
            "['खरीद']\n",
            "के\n",
            "लिए\n",
            "लिया।\n",
            "['लिया।']\n",
            "उन्होंने\n",
            "['उन्होंन']\n",
            "एक\n",
            "साल\n",
            "['साल']\n",
            "में\n",
            "वुफल\n",
            "['वुफल']\n",
            "मिलाकर\n",
            "['मिल']\n",
            "23520\n",
            "['23520']\n",
            "रुपये\n",
            "['रुपय']\n",
            "वापस\n",
            "['वापस']\n",
            "किए।\n",
            "['किए।']\n",
            "उन्होंने\n",
            "['उन्होंन']\n",
            "हर\n",
            "['हर']\n",
            "महीने\n",
            "['महीन']\n",
            "कितना\n",
            "['कित']\n",
            "पैसा\n",
            "['पैस']\n",
            "वापस\n",
            "['वापस']\n",
            "किया?\n",
            "['किया?']\n",
            "एक\n",
            "क्रिवेफट\n",
            "['क्रिवेफट']\n",
            "मैच\n",
            "['मैच']\n",
            "में\n",
            "श्रीलंका\n",
            "['श्रीलंक']\n",
            "ने\n",
            "235\n",
            "['235']\n",
            "रन\n",
            "['रन']\n",
            "बनाए।अभी\n",
            "['बनाए।अभ']\n",
            "तक\n",
            "भारत\n",
            "['भारत']\n",
            "ने\n",
            "123\n",
            "['123']\n",
            "रन\n",
            "['रन']\n",
            "बनाए\n",
            "['बन']\n",
            "हैं।\n",
            "['हैं।']\n",
            "भारत\n",
            "['भारत']\n",
            "को\n",
            "जीतने\n",
            "['जीत']\n",
            "के\n",
            "लिए\n",
            "और\n",
            "कितने\n",
            "['कित']\n",
            "रन\n",
            "['रन']\n",
            "चाहिए?\n",
            "['चाहिए?']\n",
            "भारत\n",
            "['भारत']\n",
            "को\n",
            "जीतने\n",
            "['जीत']\n",
            "वेफ\n",
            "['वेफ']\n",
            "लिए\n",
            "236\n",
            "['236']\n",
            "रन\n",
            "['रन']\n",
            "बनाने\n",
            "['बन']\n",
            "हैं।\n",
            "['हैं।']\n",
            "जीतने\n",
            "['जीत']\n",
            "के\n",
            "लिए\n",
            "और\n",
            "कितने\n",
            "['कित']\n",
            "रन\n",
            "['रन']\n",
            "शरूरी\n",
            "['शरूर']\n",
            "हैं?\n",
            "['हैं?']\n",
            "गीता\n",
            "['गीत']\n",
            "के\n",
            "बटुए\n",
            "['बटु']\n",
            "में\n",
            "368\n",
            "['368']\n",
            "रुपए\n",
            "['रुप']\n",
            "हैं।\n",
            "['हैं।']\n",
            "उसने\n",
            "['उस']\n",
            "123\n",
            "['123']\n",
            "रुपए\n",
            "['रुप']\n",
            "की\n",
            "एक\n",
            "किताब\n",
            "['किताब']\n",
            "खरीदी।\n",
            "['खरीदी।']\n",
            "उसके\n",
            "\n",
            "[]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-01cde58ff787>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# print(documents)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprocessed_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Problem'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprocessed_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, arg, na_action)\u001b[0m\n\u001b[1;32m   3981\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3982\u001b[0m         \"\"\"\n\u001b[0;32m-> 3983\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3984\u001b[0m         return self._constructor(new_values, index=self.index).__finalize__(\n\u001b[1;32m   3985\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"map\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action)\u001b[0m\n\u001b[1;32m   1158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         \u001b[0;31m# mapper is a function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1160\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-d6706ecfc010>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mlemmatized_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemmatize_stemming\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatized_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatized_token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "cQ16N8gc3eTY",
        "outputId": "66942bec-3656-4680-924b-393d25cd3633"
      },
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
        "count = 0\n",
        "for k, v in dictionary.iteritems():\n",
        "    print(k, v)\n",
        "    # count += 1\n",
        "    # if count > 10:\n",
        "    #     break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3f639672dd53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msimple_preprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'processed_docs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvEt30KKLJzI"
      },
      "source": [
        "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
        "bow_corpus[4310]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}